import pandas as pd
import json
import re
import nltk
from nltk.corpus import stopwords
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

nltk.download("stopwords")
stop_words = set(stopwords.words("english"))

# ===== ƒê·ªçc D·ªØ Li·ªáu JSON (Dataset 1) =====
def load_json_data(json_path):
    with open(json_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    # L·∫•y danh s√°ch CVE t·ª´ key "vulnerabilities"
    cve_list = data["vulnerabilities"]

    records = []
    for item in cve_list:
        cve = item["cve"]
        cve_id = cve["id"]
        base_severity = "UNKNOWN"

        # L·∫•y m·ª©c ƒë·ªô r·ªßi ro (baseSeverity)
        if "metrics" in cve and "cvssMetricV2" in cve["metrics"]:
            severity_info = cve["metrics"]["cvssMetricV2"][0]
            base_severity = severity_info["baseSeverity"]

        # L·∫•y m√¥ t·∫£ b·∫±ng ti·∫øng Anh
        description = ""
        if "descriptions" in cve:
            for desc in cve["descriptions"]:
                if desc["lang"] == "en":
                    description = desc["value"]
                    break

        # L·∫•y c√°c ƒë·∫∑c tr∆∞ng kh√°c
        access_vector = None
        if "metrics" in cve and "cvssMetricV2" in cve["metrics"]:
            access_vector = cve["metrics"]["cvssMetricV2"][0]["cvssData"]["accessVector"]

        records.append([cve_id, description, access_vector, base_severity])

    return pd.DataFrame(records, columns=["cve_id", "description", "accessVector", "severity"])


# ===== ƒê·ªçc D·ªØ Li·ªáu CSV (Dataset 2) =====
def load_csv_data(csv_path):
    df = pd.read_csv(csv_path)

    # Chuy·ªÉn ƒë·ªïi CVSS th√†nh nh√£n (LOW, MEDIUM, HIGH)
    def map_cvss_to_severity(cvss):
        if cvss < 4.0:
            return "LOW"
        elif 4.0 <= cvss < 7.0:
            return "MEDIUM"
        else:
            return "HIGH"

    df["severity"] = df["cvss"].apply(map_cvss_to_severity)
    
    # L·∫•y c√°c c·ªôt c·∫ßn thi·∫øt
    df = df[["cve_id", "summary", "access_vector", "severity"]]
    df.rename(columns={"summary": "description", "access_vector": "accessVector"}, inplace=True)

    return df


# ===== Ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n =====
def preprocess_text(text):
    text = text.lower()  # Chuy·ªÉn th√†nh ch·ªØ th∆∞·ªùng
    text = re.sub(r"\W+", " ", text)  # Lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát
    text = " ".join([word for word in text.split() if word not in stop_words])  # Lo·∫°i b·ªè stopwords
    return text


# ===== X·ª≠ l√Ω d·ªØ li·ªáu t·ªïng h·ª£p =====
def process_data(json_path, csv_path):
    df_list = []
    
    if json_path:
        df_json = load_json_data(json_path)
        df_list.append(df_json)
    
    if csv_path:
        df_csv = load_csv_data(csv_path)
        df_list.append(df_csv)
    
    if not df_list:
        raise ValueError("Both json_path and csv_path cannot be None")
    
    # N·∫øu ch·ªâ c√≥ m·ªôt dataset th√¨ kh√¥ng c·∫ßn concat
    df_combined = df_list[0] if len(df_list) == 1 else pd.concat(df_list, ignore_index=True)

     # L·ªçc b·ªè c√°c b·∫£n ghi c√≥ severity kh√¥ng h·ª£p l·ªá (UNKNOWN)
    df_combined = df_combined[df_combined["severity"].isin(["LOW", "MEDIUM", "HIGH"])]

    # Ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n
    df_combined["description"] = df_combined["description"].apply(preprocess_text)

    # One-hot encoding cho accessVector
    df_combined = pd.get_dummies(df_combined, columns=["accessVector"], dtype=int)

    # Encode severity th√†nh s·ªë (LOW=0, MEDIUM=1, HIGH=2)
    severity_mapping = {"LOW": 0, "MEDIUM": 1, "HIGH": 2}
    df_combined["severity"] = df_combined["severity"].map(severity_mapping)

    return df_combined


# ===== Ch·∫°y x·ª≠ l√Ω d·ªØ li·ªáu =====
json_path = "W1/raw_cve_data.json"  
csv_path = "W3/dataset2/merged.csv"  

df_1 = load_json_data(json_path)  # Load JSON data into df_1
df_2 = load_csv_data(csv_path)  # Load CSV data into df_2
df_final = process_data(json_path, csv_path)

# Chia train/test
X = df_final.drop(columns=["cve_id", "severity"])  # ƒê·∫∑c tr∆∞ng
y = df_final["severity"]  # Nh√£n

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Ki·ªÉm tra d·ªØ li·ªáu sau khi x·ª≠ l√Ω
print(df_final.head())

print(df_final.info())  # Xem th√¥ng tin c·ªôt v√† ki·ªÉu d·ªØ li·ªáu
print(df_final["severity"].value_counts())  # Xem ph√¢n b·ªë nh√£n

# print(df_final[df_final["severity"] == 3])  # Xem 34 m·∫´u b·ªã l·ªói (severity=3)

# print(df_1[df_1["cve_id"].isin(df_final[df_final["severity"] == 3]["cve_id"])])  # Xem th√¥ng tin c·ªßa 34 m·∫´u b·ªã l·ªói
# print(df_2[df_2["cve_id"].isin(df_final[df_final["severity"] == 3]["cve_id"])]) # Xem th√¥ng tin c·ªßa 34 m·∫´u b·ªã l·ªói


df_json = process_data(json_path, None)  # Ch·ªâ JSON
df_csv = process_data(None, csv_path)  # Ch·ªâ CSV

# Ki·ªÉm tra ph√¢n ph·ªëi severity trong t·ª´ng dataset
print("JSON dataset:")
print(df_json["severity"].value_counts(normalize=True))

print("\nCSV dataset:")
print(df_csv["severity"].value_counts(normalize=True))

# Ki·ªÉm tra c√°c gi√° tr·ªã accessVector
print("\nAccessVector JSON:", df_json.columns)
print("AccessVector CSV:", df_csv.columns)


# ƒê·ªìng nh·∫•t feature space: C·ªôt accessVector_ADJACENT_NETWORK c√≥ trong CSV nh∆∞ng kh√¥ng c√≥ trong JSON.

# Chu·∫©n h√≥a t·ª∑ l·ªá severity: Tr√°nh thi√™n v·ªã do ph√¢n ph·ªëi kh√°c nhau.

# 1Ô∏è‚É£ Chu·∫©n h√≥a d·ªØ li·ªáu (X·ª≠ l√Ω NaN)
df_combined = process_data(json_path, csv_path)
df_combined.fillna("UNKNOWN", inplace=True)  

# 2Ô∏è‚É£ T√°ch t·∫≠p d·ªØ li·ªáu theo ngu·ªìn
df_json_data = df_combined[df_combined["cve_id"].str.startswith("CVE-199")]  # Gi·∫£ s·ª≠ CVE t·ª´ JSON c√≥ pattern n√†y
df_csv_data = df_combined[~df_combined["cve_id"].str.startswith("CVE-199")]

# 3Ô∏è‚É£ Chuy·ªÉn ƒë·ªïi categorical features th√†nh d·∫°ng s·ªë
existing_columns = ["access_vector", "access_complexity", "authentication"]
available_columns = [col for col in existing_columns if col in df_combined.columns]
df_encoded = pd.get_dummies(df_combined, columns=available_columns, dtype=int)


# 4Ô∏è‚É£ Train/Test Split cho t·ª´ng dataset
X_json = df_encoded.loc[df_json_data.index].drop(columns=["cve_id", "severity"])
y_json = df_json_data["severity"]

X_csv = df_encoded.loc[df_csv_data.index].drop(columns=["cve_id", "severity"])
y_csv = df_csv_data["severity"]

X_train_json, X_test_json, y_train_json, y_test_json = train_test_split(X_json, y_json, test_size=0.2, random_state=42)
X_train_csv, X_test_csv, y_train_csv, y_test_csv = train_test_split(X_csv, y_csv, test_size=0.2, random_state=42)

# 5Ô∏è‚É£ Train model ri√™ng bi·ªát
model_json = RandomForestClassifier(n_estimators=100, random_state=42)
model_json.fit(X_train_json, y_train_json)

model_csv = RandomForestClassifier(n_estimators=100, random_state=42)
model_csv.fit(X_train_csv, y_train_csv)

# 6Ô∏è‚É£ Ki·ªÉm tra t·ª´ng model tr√™n dataset t∆∞∆°ng ·ª©ng
print("üìå Model JSON tr√™n JSON test set:")
print(classification_report(y_test_json, model_json.predict(X_test_json)))

print("üìå Model CSV tr√™n CSV test set:")
print(classification_report(y_test_csv, model_csv.predict(X_test_csv)))

# 7Ô∏è‚É£ Cross-dataset Evaluation
print("üìå Model JSON tr√™n CSV test set:")
print(classification_report(y_test_csv, model_json.predict(X_test_csv)))

print("üìå Model CSV tr√™n JSON test set:")
print(classification_report(y_test_json, model_csv.predict(X_test_json)))
